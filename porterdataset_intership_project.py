# -*- coding: utf-8 -*-
"""porterdataset intership project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z-uCXAtZUscVB8qdHqMOcdBe7NnRtQ9m
"""

import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from scipy.stats import ttest_ind
import calendar
from scipy.stats import f_oneway

"""# 1. Defining the Problem Statement, Importing Data, and Data Structure"""

pd.read_csv("/content/dataset.csv.zip")

df=pd.DataFrame(pd.read_csv("/content/dataset.csv.zip"))
df.head()

"""# Dataset shape

"""

df.shape

df.columns

"""# Data types


"""

df.dtypes

"""# Missing values

"""

df.isnull().sum()

"""# Statistical summary

"""

df.describe()

"""# 2.Data Preprocessing and Feature Engineering
# Data cleaning & Handling missing values



"""

# drop null market_id and convert data type int also convert min_item_price ,max_item_price float.
df=df.dropna(subset=['market_id'])
print(df['market_id'].isnull().sum())

# convert data type float t int also convert min_item_price ,max_item_price int to  float.
df['market_id'] = df['market_id'].astype(int)
df['min_item_price'] = df['min_item_price'].astype(float)
df['max_item_price'] = df['max_item_price'].astype(float)

# drop null value actual delivery_time.
df=df.dropna(subset=['actual_delivery_time'])
print(df['actual_delivery_time'].isnull().sum())

# Replace null value with 0 in onshift_partner ,busy_partner and total_outstanding_order columns.
df[['total_onshift_partners', 'total_busy_partners', 'total_outstanding_orders']] = (
    df[['total_onshift_partners', 'total_busy_partners', 'total_outstanding_orders']].fillna(0)
)
df

# Convert float into int data type
df['total_onshift_partners'] = df['total_onshift_partners'].astype(int)
df['total_busy_partners'] = df['total_busy_partners'].astype(int)
df['total_outstanding_orders'] = df['total_outstanding_orders'].astype(int)

# Replace null value store_primary_category with "unknown"
df['store_primary_category'] = df['store_primary_category'].fillna('Unknown')

# Replace null value order_protocol with O and convert data type float into int
df["order_protocol"]=df["order_protocol"].fillna(0)
df['order_protocol'] = df['order_protocol'].astype(int)

# convert actual_delivery_time,created_at column data type into Pandas datatime format because it is in object type
df['actual_delivery_time'] = pd.to_datetime(df['actual_delivery_time'])
df['created_at'] = pd.to_datetime(df['created_at'])

# fill null subtotal with 0 and convert float into int data tpye
df["subtotal"]=df["subtotal"].fillna(0)

# No null value in the dataset.
df.isnull().sum()

"""# Creating the target column (time taken for delivery) from order timestamp and delivery timestamp"""

# Subtract actual_delivery_time from created_at and convert its value into second and divide 60 to convert second value into minute
df['delivery_in_mins'] = (df['actual_delivery_time'] - df['created_at']).dt.total_seconds() / 60

# new columns show in data set delivery_in_mins ,in this column fill null value  with 0 and convert data type int into flot64
df["delivery_in_mins"]=df["delivery_in_mins"].fillna(0)
df["delivery_in_mins"]=df["delivery_in_mins"].astype(np.float64)

# Extracting hour from created _at column
df['order_hour'] = df['created_at'].dt.hour

# again new column show in dataset order_hour and fill null value with 0 and change data int type float64
df["order_hour"] = df["order_hour"].fillna(0)
df["order_hour"] = df["order_hour"].astype(np.float64)

# Extracting day of the week from created_at column
df['order_dayofweek'] = df['created_at'].dt.dayofweek

# new column show in dataset order_dayofweek and fill null value with 0 and change data type float into int
df["order_dayofweek"] = df["order_dayofweek"].fillna(0)
df["order_dayofweek"] = df["order_dayofweek"].astype(int)

"""# Encoding categorical columns"""

# import this lib. to encode store_primary_category column into store_primary_category_encoded
le = LabelEncoder()
df['store_primary_category_encoded'] = le.fit_transform(df['store_primary_category'])
print(df['store_primary_category_encoded'])

"""# Check if the data contains outliers"""

sns.boxplot(x='subtotal', data=df)
plt.title("Boxplot of Subtotal with outlier's")
plt.show()

"""Yes there are so many outlier's.You Can see

# Remove outliers using appropriate methods

# it is function to replace outlier Median value and then apply this function into all numberic columns
"""

# Outliers replace with median value function

def median_outliers(df, column):
    sort_column = df[column].sort_values()
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    median_val = df[column].median()
    df[column] = df[column].apply(lambda x: median_val if (x < lower or x > upper) else x)
    return df

# apply outlier function in these columns.
df = median_outliers(df, 'subtotal')
df = median_outliers(df, 'min_item_price')
df = median_outliers(df, 'max_item_price')
df = median_outliers(df, 'total_items')
df = median_outliers(df, 'order_hour')
df=  median_outliers(df, 'delivery_in_mins')
df = median_outliers(df, 'total_onshift_partners')
df = median_outliers(df, 'total_busy_partners')
df = median_outliers(df, 'total_outstanding_orders')
df = median_outliers(df, 'num_distinct_items')

# New Shape of the dataset,before clean there are (197428,14) data after cleaning (196434, 18),
# 994 rows drop and 4 new column added


df.shape

# All Column with Name and added 4 new columns delivery_in_mins,order_hour,order_dayofweek,store_primary_category_encoded.
df.columns

"""# Plot the data again to see improvements"""

# In this box Plot you Can see all outliers replace with median value in the subtotal columns  after apply median_outlier function .
sns.boxplot(x='subtotal', data=df,showfliers=False)
plt.title("Boxplot of Subtotal after replace outlier's")
plt.show()

"""#3. Data Visualization
Visualize various columns for better understanding (e.g., count plots, scatter plots)

"""

# histplot order by market_id
sns.histplot(x='market_id',data=df)
plt.title("order by market_id")
plt.show()

"""
# Distribution of Orders by Market ID:

ðŸ“Œ Dominant Markets:
Market IDs 2 and 4 have significantly higher order counts compared to the other markets. Market ID 2 has the highest number of orders, followed closely by Market ID 4. These markets represent the largest customer base or highest order frequency.

ðŸ“Œ Moderate Markets:
 Market IDs 1 and 3 have a moderate number of orders, lower than the dominant markets but still substantial.

ðŸ“Œ Less Active Markets:
 Market IDs 5 and 6 have the lowest order counts among the visualized markets, indicating a smaller customer base or lower order frequency in these areas.
# Insights:

ðŸ“Œ Market Concentration:
Order volume is heavily concentrated in Market IDs 2 and 4. Understanding the characteristics of these markets (e.g., population density, demographics, promotional activities) could provide valuable insights into drivers of order volume.

ðŸ“Œ Market Performance Disparity:
There's a clear disparity in order volume across different market IDs. This suggests that the service's penetration or popularity varies significantly by region.


# Recommendations:

ðŸ“Œ Focus on High-Performing Markets:
 Continue to invest in maintaining and potentially expanding operations in the dominant markets (2 and 4) to capitalize on the high demand. Analyze what makes these markets successful.

ðŸ“Œ Investigate Underperforming Markets:
 Conduct a thorough analysis of the less active markets (5 and 6) to understand the reasons for the lower order volume.

ðŸ“Œ Market Research:
Understanding the local demographics, competition, and customer preferences in these areas.

ðŸ“ŒMarketing Initiatives:
 Implementing targeted marketing campaigns or promotions to increase awareness and adoption of the service.

ðŸ“Œ Operational Assessment:
Evaluating if there are any logistical or service-related issues hindering growth in these markets.

ðŸ“Œ Growth Opportunities:
Explore strategies to grow order volume in the moderate markets (1 and 3). Could lessons learned from the high-performing markets be applied here?

"""

# histplot order by num_distinct_items
sns.histplot(x="num_distinct_items", data=df)
plt.title("Order by num_distinct_items ")
plt.show()

"""# Distribution of Orders by Number of Distinct Items:

ðŸ“ŒMost Frequent:
 Orders with 2 distinct items are the most frequent, with a count of approximately 65,000.

ðŸ“Œ Second Most Frequent:
Orders with 1 distinct item are the second most frequent, with a count of around 50,000.

ðŸ“Œ Moderately Frequent:
Orders with 3 distinct items are moderately frequent, with a count of approximately 42,000.

ðŸ“Œ Less Frequent:
The frequency of orders decreases as the number of distinct items increases beyond 3. Orders with 4 distinct items have around 23,000, 5 distinct items have about 12,000, and 6 distinct items have the lowest count shown, around 6000.


# Insights:

ðŸ“Œ Small Order Preference:
The majority of orders consist of a small number of distinct items (2, 1, or 3). This suggests that customers often order a few specific items rather than a wide variety.

ðŸ“Œ Focus on Core Offerings:
The high frequency of orders with 2 or 1 distinct items might indicate that certain popular or frequently paired items drive a significant portion of the order volume.

ðŸ“Œ Decreasing Variety:
As customers add more distinct items to their order, the frequency drops significantly. This could be due to factors like decision fatigue, specific needs being met with fewer items, or potentially higher costs associated with more diverse orders.

# Recommendations:

ðŸ“Œ Optimize for Popular Combinations:
 Analyze which items are most frequently ordered together (in orders with 2 or 1 distinct items). This can inform bundling strategies, cross-promotion efforts, and menu placement.

ðŸ“Œ Highlight Single-Item Orders:
Understand what types of items are frequently ordered individually. Ensure these items are prominently featured and easily accessible in the ordering interface.

ðŸ“Œ Explore Reasons for Lower Frequency of Diverse Orders:
Investigate why orders with a higher number of distinct items are less common.

ðŸ“Œ Consider Promotions for Variety:
 Experiment with promotions or discounts for orders with a higher number of distinct items to potentially encourage customers to try a wider range of offerings.


"""

df = median_outliers(df, 'delivery_in_mins')
print(df['order_protocol'].isnull().sum())
# scatter plot order_protocol and delivery_in_mins
sns.scatterplot(data=df, x='order_protocol', y='delivery_in_mins')
plt.title("Delivery Time vs Order Protocol")
plt.show()

"""# Insights

ðŸ“Œ For protocols 1 to 6, delivery times are spread out fairly evenly across a wide range (roughly between 10 to 80 minutes).

ðŸ“Œ No strong visible trend: delivery time does not drastically vary with protocols 1 to 6.

ðŸ“Œ Protocol 7 Behavior:

For order protocol 7, the number of points (orders) is much fewer compared to other protocols.

ðŸ“Œ Also, delivery times for protocol 7 seem to be lower and more compressed â€” mostly below 50 minutes.

ðŸ“Œ It suggests faster deliveries (or simply fewer data points) for protocol 7.

ðŸ“Œ The scatter is densely packed between 20 to 60 minutes across all protocols, meaning most deliveries fall in this time range.

ðŸ“Œ 0 protocol replace with null value

# Basic Questions
1.Data Structure and Overview
"""

# What is the shape of the dataset (number of rows and columns)?
'''The shape of the data is (197428, 14)Rows and columns before dropping missing values ,after clean the data ,the shape is (196434, 18)rows and column,
 there are 994 rows drop and 4 new new columns delivery_in_mins,order_hour,order_dayofweek,store_primary_category_encoded.'''

# Q.What are the data types of each column?
'''Before convert datatype
of the dataset market_id-	float64
created_at	-object
actual_delivery_time	-object
store_id	-object
store_primary_category-	object
order_protocol	-float64
total_items	-int64
subtotal-	int64
num_distinct_items	-int64
min_item_price	-int64
max_item_price	-int64
total_onshift_partners	-float64
total_busy_partners	-float64
total_outstanding_orders	-float64


datatype ,
After convert datatype


market_id	-int64
created_at	-datetime64[ns]
actual_delivery_time-	datetime64[ns]
store_id	-object
store_primary_category-	object
order_protocol-	int64
total_items-	float64
subtotal	-float64
num_distinct_items-	float64
min_item_price-	float64
max_item_price	-float64
total_onshift_partners-	float64
total_busy_partners-	float64
total_outstanding_orders	-float64
delivery_in_mins	-float64
order_hour	-float64
order_dayofweek	-int64
store_primary_category_encoded-	int64
Datatype.'''

# after clean data type
df.dtypes

'''Q.Are there any missing values in the dataset? If so, how many and in which columns?

Ans. Yes In  market_id column there are	987 values are missing,in actual_delivery_time	column 7
values missing,in store_primary_category	column 4760 value,in
order_protocol	column 995 value,In total_onshift_partners columns	16262 values,in
total_busy_partners	columns 16262 values ,in
total_outstanding_orders	columns 16262 values are missing.'''

"""# 2.Descriptive Statistics

"""

'''Q.What are the basic statistical summaries (mean, median, standard deviation) for the numerical features?

Ans.This is the statistical summary of the dataset.
market_id   order_protocol  total_items subtotal    num_distinct_items  min_item_price  max_item_price  total_onshift_partners  total_busy_partners total_outstanding_orders
count   196441.000000   196433.000000   197428.000000   197428.000000   197428.000000   197428.000000   197428.000000   181166.000000   181166.000000   181166.000000
mean    2.978706    2.882352    3.196391    2682.331402 2.670791    686.218470  1159.588630 44.808093   41.739747   58.050065
std 1.524867    1.503771    2.666546    1823.093688 1.630255    522.038648  558.411377  34.526783   32.145733   52.661830
min 1.000000    1.000000    1.000000    0.000000    1.000000    -86.000000  0.000000    -4.000000   -5.000000   -6.000000
25% 2.000000    1.000000    2.000000    1400.000000 1.000000    299.000000  800.000000  17.000000   15.000000   17.000000
50% 3.000000    3.000000    3.000000    2200.000000 2.000000    595.000000  1095.000000 37.000000   34.000000   41.000000
75% 4.000000    4.000000    4.000000    3395.000000 3.000000    949.000000  1395.000000 65.000000   62.000000   85.000000
max 6.000000    7.000000    411.000000  27100.000000    20.000000   14700.000000    14700.000000    171.000000  154.000000  285.000000
these are the statical sumaries'''
#  statistical summary
df.describe()

'''Q.What is the distribution of the categorical variables like store_primary_category and order_protocol?'''
# value count  of store_primary_category columns
print(df['store_primary_category'].value_counts())

# count plot of  store_primary_category
sns.countplot(data=df, y='store_primary_category')
plt.title("Order Count by Store Category")
plt.show()

# top 5 count plot of  store_primary_category
top5_categories = df['store_primary_category'].value_counts().head(5).index
df_top5 = df[df['store_primary_category'].isin(top5_categories)]
sns.countplot(data=df_top5, y='store_primary_category',color="pink")
plt.title("Order Count by top 5 Store Category")
plt.show()

"""# insights of distribution of the categorical variables like store_primary_category

ðŸ“Œ American:
This category has the highest order count, with approximately 19,000 orders. It represents the most frequent type of cuisine in this dataset.

ðŸ“Œ Mexican:
The Mexican store category has the second-highest order count, with roughly 17,000 orders. It's also a very popular cuisine type.

ðŸ“Œ Pizza:
Pizza stores account for the third-highest number of orders, with approximately 17,500 orders. It's another dominant category.

ðŸ“Œ Burger:
Burger stores have around 11,000 orders, placing them as the fourth most frequent category among these top 5.

ðŸ“Œ Sandwich:
Sandwich stores have the lowest order count among the top 5 categories shown, with approximately 10,000 orders.
"""

# value count  of order_protocol columns
print(df['order_protocol'].value_counts())

# hist plot of order_protocol ,count of order in order_protocol columns
sns.histplot(data=df, x='order_protocol')
plt.title("Order Count by order_protocol")
plt.show()

"""# Insights of distribution of the order_protocol
ðŸ“Œ Dominant Protocols:
Order protocols 1 and 3 have significantly higher order counts compared to the other protocols. They each account for well over 50,000 orders. This suggests that these two protocols are the most frequently used or preferred by customers.

ðŸ“Œ Moderately Used Protocols:
Protocols 2 and 5 have a moderate number of orders, in the range of 20,000 to 45,000. They are less frequent than protocols 1 and 3 but still represent a substantial portion of the orders.

ðŸ“Œ Less Frequent Protocols:
Protocols  4, 6, and 7 have very low order counts, all below 2,000 orders. This indicates that these protocols are rarely used.

ðŸ“Œ 0 in order_protocol is replace null with 0

# 3.Datetime Features
"""

'''Q.How many orders were placed each day/week/month?'''
# histplot of order by each day time
df = median_outliers(df,'order_hour')
df=median_outliers(df,'order_hour')
sns.histplot(x="order_hour", data=df,color="green")
plt.title("order count by Day")
plt.show()

"""# Insights
ðŸ“Œ Peak Ordering Time:

The highest number of orders are placed between 1 AM to 3 AM, with a sharp spike at 2 AM.

This could indicate late-night cravings or a high volume of post-dinner/lifestyle-related orders.

ðŸ“Œ Second Spike:

Another noticeable rise starts around 6 PM (18:00) and peaks at 8 PM (20:00).

This matches the typical evening dinner rush.

Very Low Activity Hours:

Between 6 AM to 2 PM, especially 8 AM to 1 PM, order counts are extremely low.

These hours likely correspond to non-meal times or low user activity.

ðŸ“Œ Consistent Late-Night Demand:

Orders remain relatively high even at midnight (0:00) and 1 AM, showing strong late-night delivery trends.

ðŸ“Œ Early Morning Dip:

There's a sharp drop starting from 4 AM to 6 AM, possibly due to restaurants being closed or lower user demand.


"""

# Countplot of order by day of week
df = median_outliers(df,'order_dayofweek')
sns.countplot(x='order_dayofweek', data=df,color="gold")
plt.title("Orders count by Day of Week")
plt.show()

"""# Insights
ðŸ“Œ Highest Order Volume:
Days 5 and 6 (likely Saturday and Sunday, assuming a 0-indexed week starting with Monday) show the highest order volume, with counts exceeding 34,000 orders. This indicates that the weekend is the busiest period for orders.

ðŸ“Œ Moderately High Order Volume:
 Days 0 and 4 (likely Monday and Friday) also have relatively high order volumes, around 27,000 to 28,000 orders.

ðŸ“Œ Lower Order Volume:
Days 1, 2, and 3 (likely Tuesday, Wednesday, and Thursday) exhibit the lowest order volumes, with counts ranging from approximately 23,500 to 26,000. These appear to be the slower days of the week.

"""

# import calender lib for fetch month from created_at columns
df["month"]=df["created_at"].dt.month
df['month_name'] = df['month'].apply(lambda x: calendar.month_abbr[x])
# count of order by month
sns.countplot(data=df,x='month_name',color="red")
plt.title("order count of Month")
plt.show()

"""# Insights
ðŸ“Œ Highest Order Volume:
February shows the highest number of orders, with a count of approximately 130000.

ðŸ“Œ Moderate Order Volume:
January has a moderate number of orders, around 68,000.
"""

'''Q.What is the distribution of order times throughout the day?'''

df['order_hour'] = df['created_at'].dt.hour
print(df['order_hour'].value_counts().sort_index())

# histplot order by each day time
df = median_outliers(df,'order_hour')
sns.histplot(x='order_hour', data=df,color="cyan")
plt.title("Distribution of Orders by Hour of Day")
plt.show()

"""# Insights
ðŸ“Œ Peak Ordering Time:

The highest number of orders are placed between 1 AM to 3 AM, with a sharp spike at 2 AM.

This could indicate late-night cravings or a high volume of post-dinner/lifestyle-related orders.

ðŸ“Œ Second Spike:

Another noticeable rise starts around 6 PM (18:00) and peaks at 8 PM (20:00).

This matches the typical evening dinner rush.

Very Low Activity Hours:

Between 6 AM to 2 PM, especially 8 AM to 1 PM, order counts are extremely low.

These hours likely correspond to non-meal times or low user activity.

ðŸ“Œ Consistent Late-Night Demand:

Orders remain relatively high even at midnight (0:00) and 1 AM, showing strong late-night delivery trends.

ðŸ“Œ Early Morning Dip:

There's a sharp drop starting from 4 AM to 6 AM, possibly due to restaurants being closed or lower user demand.

# 4.Feature Engineering
"""

'''Q.How can we create a new feature for the time taken for each delivery?'''
# Subtract actual_delivery_time from created_at and convert its value into second and divide 60 to convert second value into minute
df['delivery_in_mins'] = (df['actual_delivery_time'] - df['created_at']).dt.total_seconds() / 60
print(df[['created_at', 'delivery_in_mins']])

'''Q.How can we extract additional features from the datetime columns, such as the hour of the day or the day of the week?'''

# we fetch hour from created_at column to .dt.hour code
df['order_hour'] = df['created_at'].dt.hour
print(df['order_hour'])
# we fetch week from created_at column to .dt.dayofweek code
df['order_dayofweek'] = df['created_at'].dt.dayofweek
print(df['order_dayofweek'])

"""# 5.Exploratory Data Analysis (EDA)

"""

'''Q.What are the distribution plots for continuous variables like total_items, subtotal, min_item_price, and max_item_price?'''
df = median_outliers(df,'total_items')
sns.histplot(x='total_items',data=df,color="peru")
plt.title("count of Total Items")
plt.show()

"""# Distribution of  total_item

ðŸ“Œ Discrete Peaks:
The distribution shows distinct peaks at integer values of total items (1, 2, 3, 4, 5, 6, and 7). This confirms its discrete nature.

ðŸ“Œ Most Frequent Order Sizes:
The highest peaks are at 2 and 3 total items, indicating that the most frequent order sizes contain either 2 or 3 items. Orders with 1 item are also quite common.

ðŸ“Œ Decreasing Frequency with Increasing Items:
 As the number of total items increases beyond 3, the frequency of orders generally decreases. Orders with 6 or 7 items are relatively rare compared to smaller orders.

ðŸ“Œ Relatively Limited Range:
The number of total items in an order is mostly concentrated within the range of 1 to 7.
"""

# histplot order by subtotal
df = median_outliers(df, 'subtotal')
sns.histplot(df['subtotal'], bins=50,color="coral")
plt.title(" count of Subtotal")
plt.show()

"""# Distribution of subtotal

ðŸ“Œ  Primary Peak:
The most significant peak is around a subtotal of approximately $2100. This indicates that a large number of orders fall within this price range.

ðŸ“Œ Secondary Peak:
There's a smaller but still noticeable peak around a subtotal of approximately $1200. This suggests another common price point for orders, albeit less frequent than the primary peak.

ðŸ“Œ Right Skewness:
The distribution has a long right tail, indicating that while many orders are clustered around the two peaks, there are also a significant number of orders with much higher subtotals, extending up to around $5500 and beyond.

ðŸ“Œ Range of Subtotals:
Order subtotals range from very low values (close to $0) to high values (over $5500).


"""

# histplot order by min_item_price
df = median_outliers(df, 'min_item_price')
sns.histplot(df['min_item_price'], bins=50,color="sienna")
plt.title("count of Min Item Price")
plt.show()

"""# Distribution of min_item_price

ðŸ“Œ Primary Peaks:
The most prominent peaks seem to be around:
$150 - $250
$550 - $650
$750 - $850

ðŸ“Œ Other Local Peaks:
There are also smaller peaks or areas of higher frequency around other price points, indicating other common minimum item prices.

ðŸ“Œ Right Skewness:
 The distribution shows a right skew, with a tail extending towards higher minimum item prices (up to around $1800). This indicates that while many orders have a relatively low minimum item price, there are also orders where the cheapest item is quite expensive.

ðŸ“Œ Range:
The minimum item price in orders ranges from very low values (close to $0) to around $1800.

"""

# histplot order by max_item_price
df = median_outliers(df, 'max_item_price')
sns.histplot(df['max_item_price'], bins=50,color="violet")
plt.title("Distribution of Max Item Price")
plt.show()

"""# Distribution of max_item_price
ðŸ“Œ Multi-Modal Distribution:
Similar to the minimum item price, the maximum item price also exhibits a multi-modal distribution, with several prominent peaks, indicating common maximum price points for items within orders.

ðŸ“Œ Primary Peaks:
The most significant peaks appear around:
$1100 - $1200: This is the most dominant peak, suggesting that a large number of orders have a maximum item price in this range.
$800 - $1000

ðŸ“Œ Other Notable Peaks:
There are other less prominent but still noticeable peaks around:
$400 - $500
$1400 - $1500

ðŸ“Œ Right Skewness:
The distribution shows a right skew, with a tail extending towards higher maximum item prices (up to around $2000 and beyond). This indicates that while many orders have a relatively lower maximum item price, there's a significant number of orders where the most expensive item is quite costly.

ðŸ“Œ Range:
The maximum item price in orders ranges from relatively low values (around $100) to high values (over $2000).
"""

'''Q.What are the count plots for categorical variables like store_primary_category and order_protocol?'''
# top 5 categorical variables count order by  top 5 categorical variables
top5_categories = df['store_primary_category'].value_counts().head(5).index
df_top5 = df[df['store_primary_category'].isin(top5_categories)]
sns.countplot(data=df_top5, y='store_primary_category',color="pink")
plt.title(" Distribution of Store Category")
plt.show()

"""# counts of top 5 categorical variable.
ðŸ“Œ American:
This category has the highest order count, with approximately 19,000 orders. It represents the most frequent type of cuisine in this dataset.

ðŸ“Œ Mexican:
The Mexican store category has the second-highest order count, with roughly 17,000 orders. It's also a very popular cuisine type.

ðŸ“Œ Pizza:
Pizza stores account for the third-highest number of orders, with approximately 17,500 orders. It's another dominant category.

ðŸ“Œ Burger:
Burger stores have around 11,000 orders, placing them as the fourth most frequent category among these top 5.

ðŸ“Œ Sandwich:
Sandwich stores have the lowest order count among the top 5 categories shown, with approximately 10,000 orders.
"""

# histplot of counts of protocol variable.
sns.histplot(data=df, x='order_protocol')
plt.title(" Distribution of order_protocol")
plt.show()

"""# Count of order_protocol

ðŸ“Œ Dominant Protocols:
Order protocols 1 and 3 have significantly higher order counts compared to the other protocols. They each account for well over 50,000 orders. This suggests that these two protocols are the most frequently used or preferred by customers.

ðŸ“Œ Moderately Used Protocols:
Protocols 2 and 5 have a moderate number of orders, in the range of 20,000 to 45,000. They are less frequent than protocols 1 and 3 but still represent a substantial portion of the orders.

ðŸ“Œ Less Frequent Protocols:
Protocols  4, 6, and 7 have very low order counts, all below 2,000 orders. This indicates that these protocols are rarely used.

ðŸ“Œ 0 in order_protocol is replace null with 0

# 6.Missing Values Handling
"""

'''Q.How can we handle missing values in the dataset, especially for important columns like store_primary_category?
We handle missing value in store_primary_category null value replace with "unknown" with this python code, df['store_primary_category'] = df['store_primary_category'].fillna('Unknown') and then replace null protocol with 0 and  drop market_id null value because null values contain no location of restaurent. Drop null delivery_time because with null value  the result of the analysis will not be correct '''

"""# 7.Correlation Analysis

"""

'''Q.What are the Pearson and Spearman correlation coefficients between numerical features (e.g., total_items,
subtotal, min_item_price, max_item_price)? What do these correlations suggest?'''

# variable assign
continuous_vars = ['total_items', 'subtotal', 'min_item_price', 'max_item_price']

# it give the Pearson correlation coefficients of continuous_vars
pearson_corr = df[continuous_vars].corr(method='pearson')
print( pearson_corr)

"""# insights
ðŸ“Œ Range: -1 to +1

+1: Perfect positive linear relationship

-1: Perfect negative linear relationship

0: No linear relationship


 ðŸ“Œ  total_items vs. subtotal â†’ Moderate Positive Correlation (0.56)
Indicates that as the number of items increases, the subtotal tends to increase, but not linearly. Possibly due to varying item prices (e.g., adding multiple cheap items doesn't raise the total much).

ðŸ“Œ  total_items vs. min_item_price â†’ Moderate Negative Correlation (-0.48)
Suggests that when orders have many items, the cheapest item in the order tends to be lower in price â€” likely due to bulk or combo-type orders where cheaper items dominate.

ðŸ“Œ  subtotal vs. max_item_price â†’ Moderate Positive Correlation (0.47)
Indicates that expensive individual items contribute significantly to the subtotal, even if total item count is low (e.g., one or two premium items).

ðŸ“Œ  min_item_price vs. max_item_price â†’ Moderate Positive Correlation (0.42)
In orders with high-priced items, even the cheaper items tend to be somewhat expensive â€” possibly indicating high-end or premium order types.

ðŸ“Œ  total_items vs. max_item_price â†’ No Significant Correlation (0.008)
The number of items in an order doesn't influence the price of the most expensive item, meaning expensive items are ordered in both small and large item-count orders.

ðŸ“Œ  subtotal vs. min_item_price â†’ Very Low Correlation (0.004)
The cheapest itemâ€™s price has little to no influence on the overall order value.


"""

# it give the  spearman correlation coefficients of continuous_vars
spearman_corr = df[continuous_vars].corr(method='spearman')
print( spearman_corr)

"""# Insights

ðŸ“Œ total_items vs. subtotal â†’ Moderate-High Positive Correlation (0.61)
Stronger than the Pearson result (0.56), indicating a more consistent upward trend â€” even if not strictly linear â€” where more items usually mean higher total cost.

ðŸ“Œ total_items vs. min_item_price â†’ Moderate Negative Correlation (-0.53)
Even more pronounced than in Pearson â€” when many items are ordered, the lowest-priced item tends to be cheaper. Suggests bulk/low-cost combinations are common in large orders.

ðŸ“Œ subtotal vs. max_item_price â†’ Moderate Positive Correlation (0.53)
Confirms that high-value items heavily influence the order total â€” Spearman reveals a stronger pattern than Pearson here (0.47).

â†—ðŸ“Œ min_item_price vs. max_item_price â†’ Moderate Positive Correlation (0.39)
Still a positive trend â€” premium orders tend to have both high min and max prices â€” but slightly weaker than in Pearson (0.42), suggesting more variability.

ðŸ“Œtotal_items vs. max_item_price â†’ Very Weak Correlation (0.02)
Again, no significant trend: expensive items are ordered regardless of how many items are in the order.

ðŸ“Œ subtotal vs. min_item_price â†’ Negligible Correlation (0.009)
Reinforces the idea that cheap items donâ€™t move the subtotal much.

# 8.Multivariate Analysis
"""

'''Q.How do multiple factors (e.g., market_id, store_primary_category, order_protocol) together influence the subtotal or delivery time?'''

# boxplot of subtotal by Market ID and Order Protocol
df=median_outliers(df,'subtotal')
sns.boxplot(data=df, x='market_id', y='subtotal', hue='order_protocol',showfliers=False)
plt.title("Subtotal by Market ID and Order Protocol (No Outlier Dots)")
plt.show()

"""# Insights

ðŸ“Œ Variation by Market:

Subtotals vary considerably across markets.

E.g., Market 4 tends to have higher subtotals, while Market 6 leans lower across most protocols.

ðŸ“Œ Indicates potential differences in cost-of-living, restaurant pricing, or order size trends by region.

ðŸ“Œ Protocol 3, 4, 6 Show Similar Subtotal Ranges Across Markets:

ðŸ“Œ These more defined protocols appear consistent, suggesting they might represent standard digital ordering methods like apps or pre-scheduled deliveries.

"""

top5_categories = df['store_primary_category'].value_counts().head(5).index
df_top5 = df[df['store_primary_category'].isin(top5_categories)]
df=median_outliers(df,'delivery_in_mins')
sns.boxplot(data=df_top5,x='store_primary_category',y='delivery_in_mins',hue='order_protocol',showfliers=False)
plt.title("Delivery Time Distribution by Top 5 Categories and Order Protocol")
plt.show()

"""
# Insights
ðŸ“Œ Order Protocol 0 (Null Values):


ðŸ“Œ Mexican continues to show the lowest and most consistent delivery times, across protocols.

ðŸ“Œ Pizza, Burger, and American categories show higher medians and wider variability, suggesting operational inconsistency or location-based issues.

ðŸ“Œ Impact of Protocols on Delivery Time:

ðŸ“Œ Protocols like 2, 3, and 6 tend to have higher delivery time medians and wider spread, especially for pizza and burger.

ðŸ“Œ Protocols 1 and 4 seem more stable and efficient across most categories.

ðŸ“Œ Sandwich is relatively consistent across all protocols"""

df=median_outliers(df,'delivery_in_mins')
sns.boxplot(data=df_top5,x='store_primary_category',y='delivery_in_mins',hue='order_protocol',showfliers=False)
plt.title("Delivery Time Distribution by Top 5 Categories and Order Protocol")
plt.show()

"""# 9.Outlier Analysis"""

'''Q.Are there any outliers in the dataset? Which method can be used to identify and handle these outliers?
Yes there are lot of outliers in subtotal,total_item and more columns .I use IQR method and boxplot to identify and handle these outliers.
In IQR method we find lower and upper value then distribute date into 25,50,75 percentile then find IQR=Q3-Q1
and lower =Q1-1.5*IQR and upper=Q3-1.5*IQR and then replace outliers with median value '''

"""# 10.Categorical Feature Encoding"""

'''How can we encode categorical variables like store_primary_category and order_protocol for further analysis?
we encode categorical variable into numrical format by using from sklearn.preprocessing import LabelEncoder this lib. and this python code'''

df['store_primary_category_encoded'] = le.fit_transform(df['store_primary_category'])

# encode order_protocol
le = LabelEncoder()
df['order_protocol_encoded'] = le.fit_transform(df['order_protocol'])

"""# 11.Advanced Feature Engineering"""

'''Q.Can we create a feature based on the availability of delivery partners, such as a ratio of total_busy_partners to total_onshift_partners?'''
df['partner_available_ratio'] = df['total_busy_partners'] / df['total_onshift_partners']
df['partner_available_ratio'].fillna(0)
df=median_outliers(df,'partner_available_ratio')
sns.scatterplot(data=df, x='delivery_in_mins', y='partner_available_ratio',)
plt.xlabel('Delivery Time (minutes)')
plt.ylabel('Partner Availability Ratio')
plt.title('Delivery Time vs. Partner Availability')
plt.show()

"""# Insights
ðŸ“Œ  Dense Cluster Around Ratio = 1:
There's a very dense horizontal band of points clustered around a partner_available_ratio of 1. This confirms that a large number of orders occurred when the number of busy partners was equal to the number of on-shift partners.

ðŸ“Œ  Spread Across Delivery Times:
This dense cluster around a ratio of 1 spans a wide range of delivery times, from relatively short (around 10 minutes) to quite long (up to 80 minutes). This suggests that even when partner utilization is very high, delivery times can still vary significantly, likely due to other factors.


ðŸ“Œ Higher Ratios (Above 1):
There are points scattered with a partner_available_ratio above 1, indicating instances where busy partners outnumbered on-shift partners. These points also show a spread of delivery times.


ðŸ“Œ Lower Ratios (Below 1):
Similarly, there are points with a partner_available_ratio below 1, indicating periods with more available partners. These points also exhibit a range of delivery times.
"""

''' Q.How do engineered features like order time of day or week enhance the predictive power or insights of the analysis?
 ANS.
 1. Identify Demand Patterns (Operational Efficiency)
order_hour helps you spot peak vs. off-peak hours.

order_dayofweek reveals weekend vs. weekday trends.

ðŸ“Œ Helps businesses optimize staffing, delivery partner availability, and inventory based on actual demand patterns.

2. Improve Delivery Time Predictions
Certain hours or days might correlate with traffic, restaurant load, or partner shortages.

ðŸ“Œ Including these features can boost prediction accuracy for delivery times when building ML models.

3. Understand Customer Behavior
Time-based features help analyze when users are most likely to order specific categories (e.g., pizza at night, sandwiches at lunch).

ðŸ“Œ Inform targeted promotions or notifications (e.g., send a burger coupon on Friday evenings).

4. Reveal Performance Bottlenecks
Higher delivery delays during specific hours/days may indicate operational bottlenecks, like:

Low partner availability

High outstanding order count

ðŸ“Œ Guides load balancing strategies and partner scheduling.

5. Segmentation and Personalization
Users might have recurring behavior patterns (e.g., ordering lunch daily at 1 PM).

ðŸ“Œ Enables personalized features like order suggestions, dynamic pricing, or loyalty perks.'''

"""# 12.Advanced Visualization

"""

'''Q.Use advanced visualization techniques (e.g., heatmaps, pair plots) to explore relationships between multiple variables simultaneously.'''
df = median_outliers(df, 'total_items')
df = median_outliers(df, 'subtotal')
df = median_outliers(df, 'min_item_price')
df = median_outliers(df, 'delivery_in_mins')
df = median_outliers(df, 'partner_available_ratio')
df = median_outliers(df, 'order_hour')
df = median_outliers(df, 'num_distinct_items')



cols_set1 = ['total_items', 'num_distinct_items', 'subtotal', 'min_item_price','delivery_in_mins', 'partner_available_ratio', 'order_hour', 'order_dayofweek']
sns.pairplot(df[cols_set1], plot_kws={'color': 'royalblue'})
plt.show()

"""# insights
ðŸ“Œ Insights from the Pair Plot:

ðŸ“Œ total_items vs. subtotal:
There appears to be a positive correlation, as we observed earlier. Generally, orders with more total items tend to have a higher subtotal. The relationship isn't perfectly linear, suggesting that the price per item might vary.

ðŸ“Œ num_distinct_items vs. subtotal:
Similar to 'total_items', there's a positive correlation. Orders with a greater variety of distinct items tend to have a higher subtotal. This makes sense as more unique items likely contribute to a higher overall cost.

ðŸ“Œtotal_items vs. num_distinct_items:
There's a strong positive correlation. Orders with more total items also tend to have a higher number of distinct items. However, the scatter plot shows some spread, indicating that some large orders might consist of multiple units of the same few items.

ðŸ“Œ min_item_price vs. subtotal:
There's a positive correlation, although it's less strong than with 'total_items' or 'num_distinct_items'. Orders with a higher minimum item price tend to have a higher subtotal, but there's considerable variability.

ðŸ“Œ delivery_in_mins vs. other numerical variables:
There doesn't appear to be a strong linear correlation between 'delivery_in_mins' and 'total_items', 'num_distinct_items', 'subtotal', or 'min_item_price'. This suggests that the number or price of items in an order might not be a primary driver of delivery time within this dataset. However, further analysis (e.g., considering non-linear relationships or other factors) might reveal more subtle influences.  There's no clear linear correlation between 'delivery_in_mins' and 'partner_available_ratio'. This is interesting and suggests that the overall availability of partners, as represented by this ratio, might not directly translate to faster or slower delivery times at an aggregate level. There could be other factors at play, such as the geographical distribution of partners or the efficiency of the dispatch system.

ðŸ“Œ Relationships with 'order_hour':
The scatter plots of 'delivery_in_mins', 'subtotal', 'total_items', 'num_distinct_items', and 'min_item_price' against 'order_hour' don't show strong linear trends. However, we can see the distribution of orders across different hours in the histograms along the diagonal, confirming the peak ordering times we observed earlier.

ðŸ“Œ Relationships with 'order_dayofweek':
Similar to 'order_hour', the scatter plots against 'order_dayofweek' don't reveal strong linear correlations with the numerical variables. The histograms along the diagonal show the distribution of orders across the days of the week, again confirming the weekend peak.

ðŸ“Œ Histograms (Diagonal): The histograms along the diagonal provide the univariate distributions we've discussed previously for each of these variables.



"""

sns.heatmap(df[cols_set1].corr(),annot=True)
plt.show()

"""# insights

ðŸ“ŒTotal Items vs. Number of Distinct Items

Correlation: ~0.83 â€“ Very strong positive correlation.

ðŸ“Œ As total items increase, distinct items also increase. Larger orders tend to include a variety of different items.

Total Items / Distinct Items vs. Subtotal

Correlation: ~0.54â€“0.56 â€“ Moderate positive correlation.

ðŸ“Œ Higher order volume and item variety are associated with higher order subtotals, as expected.

Subtotal vs. Min/Max Item Price

Min Price: Low negative correlation (~-0.48).

Max Price: Moderate positive correlation (~0.47).

ðŸ“Œ Subtotal is influenced more by expensive (max priced) items than by cheaper (min priced) items.

Delivery Time vs. Subtotal / Item Metrics

Correlation: Weak (~0.13â€“0.18).

ðŸ“Œ Delivery time is only slightly influenced by order size or subtotal. Larger/more expensive orders aren't delayed much more than others.

Delivery Time vs. Partner Availability

Very weak positive correlation (~0.06).

ðŸ“Œ Slight trend where more available partners help speed up deliveries, but the effect is minimal â€” suggesting other factors like traffic or order density are more important.

Time-based Variables (Order Hour, Day of Week)

Very weak correlations (around -0.11 to 0.05).

ðŸ“Œ No strong dependency between time variables and subtotal, item counts, or delivery time. Demand patterns seem relatively steady across different times and days.



"""

''''Q.How do interactions between categorical variables (e.g., store_primary_category * order_protocol) affect the delivery time?'''
top5_categories = df['store_primary_category'].value_counts().head(5).index
df_top5 = df[df['store_primary_category'].isin(top5_categories)]
sns.boxplot(data=df_top5,x="order_protocol",y='delivery_in_mins', hue='store_primary_category',showfliers=False)
plt.title("Delivery Time by Store Category and Order Protocol")
plt.show()

"""# insights
ðŸ“Œ Order Protocol 0 (Null Values):

ðŸ“Œ Mexican continues to show the lowest and most consistent delivery times, across protocols.

ðŸ“Œ Pizza, Burger, and American categories show higher medians and wider variability, suggesting operational inconsistency or location-based issues.

ðŸ“Œ Protocols like 2, 3, and 6 tend to have higher delivery time medians and wider spread, especially for pizza and burger.

ðŸ“Œ Protocols 1 and 4 seem more stable and efficient across most categories.

ðŸ“Œ Sandwich is relatively consistent across all protocols.



"""

'''Q.Perform statistical tests to determine if there are significant differences in delivery times between different groups
(e.g., different restaurant categories or order protocols).'''


'''The t-test works by calculating a t-statistic,
which essentially measures the difference between
the group means relative to the variability within the groups.
It then calculates the probability (p-value) of observing such a difference
if there were actually no real difference between the groups.
The standard t-test is for comparing two groups.'''

# check the category american', 'burger in the dataset
df_filtered = df[df['store_primary_category'].isin(['american', 'burger'])]

#group the dataset categorical american', 'burger of its delivery time
group_american = df_filtered[df_filtered['store_primary_category'] == 'american']['delivery_in_mins']
group_burger = df_filtered[df_filtered['store_primary_category'] == 'burger']['delivery_in_mins']

print(group_american)
print(group_burger)

#check there is any null value
print(group_american.isnull().sum())
print(group_burger.isnull().sum())
# replace outlier in delivery_in_mins columns
df=  median_outliers(df, 'delivery_in_mins')
# mean of american', 'burger delivery time
print(group_burger.mean())
print(group_american.mean())
# box plot of american', 'burger delivery time
sns.boxplot(data=df_filtered, x='store_primary_category', y='delivery_in_mins')
plt.title("Delivery Time by american', 'burger category")
plt.show()



# Perform independent two-sample t-test
t_statistic, p_value = ttest_ind(group_american, group_burger)

print(t_statistic)
print(p_value)

"""# insights
The t-test is a statistical hypothesis test that compares the means of two independent groups. In my case, the two groups are:

ðŸ“Œ Group 1: Delivery times for orders from "American" restaurants.
ðŸ“Œ Group 2: Delivery times for orders from "Burger" restaurants.

ðŸ“Œ Null Hypothesis (H0):
The t-test starts with a null hypothesis. In this scenario, the null hypothesis is that there is no difference in the average delivery times between "American" and "Burger" restaurants. In other words, the true population means are equal.


ðŸ“Œ Alternative Hypothesis (H1):
The alternative hypothesis is what you're trying to find evidence for. Here, the alternative hypothesis is that there is a difference in the average delivery times between the two restaurant categories. The true population means are not equal.

ðŸ“Œ T-statistic:
The t-statistic is a calculated value that measures how far the sample means are from each other in terms of the standard error.
A larger absolute value of the t-statistic generally indicates a greater difference between the sample means.
The t-statistic is used to calculate the p-value.


ðŸ“Œ P-value:
The p-value is the probability of observing the obtained sample results (or more extreme results) if the null hypothesis were true.
It's a measure of how likely the observed difference in sample means is due to random chance.
A small p-value suggests that the observed difference is unlikely to be due to chance, providing evidence against the null hypothesis.


ðŸ“Œ T-statistic: 1.778
This value indicates that there is some difference between the sample means of the two groups.



ðŸ“Œ P-value: 0.0755
This is the crucial value for decision-making. It means that if there were truly no difference in average delivery times between "American" and "Burger" restaurants

ðŸ“Œ If p-value â‰¤ 0.05 â†’ You reject the null hypothesis (evidence suggests there is a difference).

ðŸ“Œ If p-value > 0.05 â†’ You fail to reject the null hypothesis (not enough evidence to say there is a difference).

# Conclusion
We performed a two-sample t-test to compare delivery times between American and burger  categories.
The results showed a significant difference Based on the independent two-sample t-test (p = 0.075), there is no significant difference in delivery times between the American and Burger categories. Both categories exhibit very similar average delivery durations, around 45 minutes.
"""

#  Perform one-way ANOVA two or more group
print(df['order_protocol'].unique())

# One-Way ANOVA test
# Compares the means of a numerical variable for three or more groups

df=  median_outliers(df, 'delivery_in_mins')
group_1 = df[df['order_protocol'] == 1]['delivery_in_mins']
group_2 = df[df['order_protocol'] == 2]['delivery_in_mins']
group_3 = df[df['order_protocol'] == 3]['delivery_in_mins']
group_4 = df[df['order_protocol'] == 4]['delivery_in_mins']
group_5 = df[df['order_protocol'] == 5]['delivery_in_mins']
group_6 = df[df['order_protocol'] == 6]['delivery_in_mins']
group_7 = df[df['order_protocol'] == 7]['delivery_in_mins']
sns.boxplot(data=df, x='order_protocol', y='delivery_in_mins')
plt.title("Delivery Time by Order Protocol")
plt.show()
print(group_1.mean())
print(group_2.mean())
print(group_3.mean())
print(group_4.mean())
print(group_5.mean())
print(group_6.mean())
print(group_7.mean())

# Perform one-way ANOVA
f_statistic, p_value = f_oneway(group_1, group_2, group_3, group_4, group_5, group_6, group_7)

print(f_statistic)
print(p_value)

"""# Insights

ðŸ“Œ I am checking  delivery times differ significantly across different order_protocol categories (1 to 7).
I did this using a one-way ANOVA (because I have more than 2 groups).


ðŸ“Œ Mean Delivery Times per Order Protocol:

ðŸ“Œ Protocol 1: 46.50 mins

ðŸ“Œ Protocol 2: 44.79 mins

ðŸ“Œ Protocol 3: 44.62 mins

ðŸ“Œ Protocol 4: 44.28 mins

ðŸ“Œ Protocol 5: 43.44 mins

ðŸ“Œ Protocol 6: 51.49 mins (highest)

ðŸ“Œ Protocol 7: 41.37 mins (lowest)

ðŸ“Œ ANOVA Test Results:

ðŸ“Œ  F-statistic = 258.47

ðŸ“Œ p-value = 0.0 (effectively < 0.001)


ðŸ“Œ Since the p-value is far below 0.05, you reject the null hypothesis.

This means:
ðŸ”¥ There is a statistically significant difference in delivery times between different order protocols.




ðŸ“Œ F-statistic = 258.47 (which is very large ðŸš€)  ,  p-value = 0.0

ðŸ“Œ Why is the p-value 0.0?

ðŸ“Œ It means the p-value is extremely small, so small that Python is rounding it to 0 for display.

ðŸ“Œ Null Hypothesis (Hâ‚€):
 All groups (protocols) have the same mean delivery time.

ðŸ“Œ  Alternative Hypothesis (Hâ‚):
 At least one group's mean delivery time is different.

ðŸ“Œ  Since p-value â‰ˆ 0 < 0.05, you reject the null hypothesis.

ðŸ“Œ There are significant differences in delivery times across different order protocols.

# Conclusion
A one-way ANOVA test was conducted to examine the differences in delivery times across different order protocols.
The results showed a very high F-statistic of 258.47 and a p-value approximately equal to 0, indicating highly significant differences.
Thus, we reject the null hypothesis and conclude that delivery times significantly vary based on the order protocol used.
This suggests that some protocols are consistently faster or slower than others.

# EDA Report: Exploratory Data Analysis

1.  Data Overview

* The dataset contains information related to delivery orders, including market ID, order creation and delivery times, order details (items, subtotal), category information, and delivery logistics.

* The dataset has 197428 rows and 14 columns,after cleaning the dataset has (196434, 18) rows and columns

* Key variables include:
  ðŸ“Œ 'market_id':  Categorical variable representing different market areas.

  ðŸ“Œ  `created_at`, `actual_delivery_time`: Timestamps for order creation and delivery.

  ðŸ“Œ  `total_items`, `num_distinct_items`, `subtotal`:  Variables describing the order contents and cost.

  ðŸ“Œ `store_primary_category`:  Categorical variable indicating the type of restaurant.

  ðŸ“Œ `delivery_in_mins`: Numerical variable representing the delivery time.

  ðŸ“Œ `order_protocol`: Categorical variable specifying how the order was place

  ðŸ“Œ `order_hour`, `order_dayofweek`:  Temporal features derived from the order timestamp.


2.  Univariate Analysis

* **Numerical Variables:**
  ðŸ“Œ`total_items`, `num_distinct_items`, `subtotal`, `min_item_price`,

  ðŸ“Œ `max_item_price`:  
  These variables exhibit right-skewness, indicating a concentration of orders with smaller values and a tail of orders with larger values. Outliers are present in these variables.

  ðŸ“Œ `delivery_in_mins`:  
  The distribution is generally right-skewed, suggesting that most orders are delivered within a certain time frame, but some take considerably longer.

  ðŸ“Œ `order_hour`: Orders are concentrated during peak hours (evening).

  ðŸ“Œ  `order_dayofweek`:  Higher order volume is observed towards the weekend.

* **Categorical Variables:**

  ðŸ“Œ `market_id`:  Order distribution varies across markets, with some markets having significantly higher order volumes.

  ðŸ“Œ  `store_primary_category`:  The dataset contains a variety of restaurant categories, with some categories being more frequent than others.

  ðŸ“Œ  `order_protocol`:  Different order protocols are used, with varying frequencies.

3.  Bivariate Analysis

* **Correlations:**
  ðŸ“Œ  `total_items` and `subtotal` are positively correlated, indicating that orders with more items tend to have higher subtotals.

  ðŸ“Œ `num_distinct_items` and `subtotal` are also positively correlated, suggesting that orders with a greater variety of items tend to have higher subtotals.

  ðŸ“Œ `total_items` and `num_distinct_items` are strongly positively correlated.

* **Delivery Time:**
    * Delivery time (`delivery_in_mins`) does not show strong linear correlations with order-related numerical features.

    * Delivery times vary across different `store_primary_category` and `order_protocol`.

* **Temporal Patterns:**
    * Order volume varies significantly by `order_hour` and `order_dayofweek`.

# Data Preprocessing Report

ðŸ“Œ Missing Value Handling:
Missing values were checked for in each column.
The following columns had   market_id column there are	987 values are missing,in actual_delivery_time	column 7 ,values missing, in store_primary_category ,column 4760 value missing,in order_protocol	column 995 value missing ,In total_onshift_partners columns	16262 values missing ,in total_busy_partners	columns 16262 values missing,in total_outstanding_orders	columns 16262 values are missing.



ðŸ“Œ Feature Engineering:
delivery_in_mins was calculated from created_at and actual_delivery_time.
order_hour and order_dayofweek were extracted from created_at.
order_hour was calculated from food_prep_time, first_mile_distance, and last_mile_distance.


ðŸ“Œ Encoding:
store_primary_category and order_protocol were encoded using Label Encoding.


ðŸ“Œ Outlier Removal
Outliers were replaced with median from using IQR method.



# Insights and Recommendations
ðŸ“Œ Delivery Time: Delivery time is very important. We need to study it more to understand what makes it faster or slower. (Instead of "key drivers," use "what makes it faster or slower")

ðŸ“Œ Since simple linear correlations with order-related features are weak, explore non-linear relationships and interactions between variables.

ðŸ“Œ Analyze the impact of store_primary_category and order_protocol on delivery times, as significant differences were observed.

ðŸ“Œ Consider time-based factors (e.g., peak hours, day of week) and external factors (e.g., traffic, weather) in delivery time modeling.

ðŸ“Œ Order Characteristics and Revenue:
Orders with more items and a greater variety of items tend to generate higher revenue.

ðŸ“Œ Promote strategies to encourage customers to order more items and try different items.

ðŸ“Œ Analyze popular item combinations to optimize bundling and cross-selling.

ðŸ“Œ Market-Specific Strategies:
Order volume varies across different market areas.

ðŸ“Œ Develop market-specific strategies based on local demand patterns and characteristics.


ðŸ“Œ Improve the efficiency of order processing and delivery protocols to reduce delivery times.
"""